humanoid-ppo:
    env: Humanoid-v4
    run: PPO
    stop:
        sampler_results/episode_reward_mean: 6000
        timesteps_total: 40000000
    config:
        # Works for both torch and tf.
        framework: torch
        gamma: 0.995
        kl_coeff: 1.0
        num_sgd_iter: 20
        lr: .0001
        sgd_minibatch_size: 32768
        train_batch_size: 320000
        model:
            free_log_std: true
        use_gae: false
        num_workers: 10
        num_envs_per_worker: 20
        num_gpus: 1
        batch_mode: complete_episodes
        observation_filter: MeanStdFilter
